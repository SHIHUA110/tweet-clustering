{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Topic Clustering to predict a Tweet's # of Favorites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does a tweet's content affect the number of \"favorites\" it receives? We will use doc2vec (extension of word2vec) to embed tweets in a vector space. Then we will cluster these vectors using K-Means and see if those clusters carry any explanatory power for favorite counts\n",
    "\n",
    "In terms of process flow:\n",
    "MongoDB -> Gensim Python Package -> K-Means Clustering -> StatsModel OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [""]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import string,logging,re\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query MongoDB and Process Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read in tweets from a MongoDB holding that last ~3200 tweets posted by a user. In this example, exploring the most consequential twitter handle: @RealDonaldTrump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "c=MongoClient()\n",
    "tweets_data=c.twitter.tweets\n",
    "data=[x['text'] for x in tweets_data.find({'user.name':'Donald J. Trump'},{'text':1, '_id':0})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our tweet processing:\n",
    "\n",
    "    Split into words\n",
    "    \n",
    "    Remove all punctuation\n",
    "    \n",
    "    Remove tokens that are only numeric or only 1 character in length\n",
    "    \n",
    "    Remove links and '&' symbols\n",
    "    \n",
    "    Tag Each Sentence with an integer id for our doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['use', 'social', 'media', 'presidential', 'itâ€™s', 'modern', 'day', 'presidential', 'make', 'america', 'great', 'again'], tags=[10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "tokenized_tweets=list()\n",
    "for d in data:\n",
    "     text=[word for word in d.lower().split() if word not in stopwords.words(\"english\")]\n",
    "     text=[t.translate(table) for t in text]\n",
    "     clean_tokens=[]\n",
    "     for token in text:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>1 and '@' not in token and token!='amp' and 'https' not in token:\n",
    "            clean_tokens.append(token)\n",
    "     tokenized_tweets.append(clean_tokens)\n",
    "tag_tokenized=[gensim.models.doc2vec.TaggedDocument(tokenized_tweets[i],[i]) for i in range(len(tokenized_tweets))]\n",
    "tag_tokenized[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If we can help little #CharlieGard, as per our friends in the U.K. and the Pope, we would be delighted to do so.', 'At some point the Fake News will be forced to discuss our great jobs numbers, strong economy, success with ISIS, the border &amp; so much else!', 'Will be speaking with Italy this morning!', 'Spoke yesterday with the King of Saudi Arabia about peace in the Middle-East. Interesting things are happening!', 'Will be speaking with Germany and France this morning.']\n",
      "3219\n",
      "\n",
      "\n",
      "[['help', 'little', 'charliegard', 'per', 'friends', 'uk', 'pope', 'would', 'delighted', 'so'], ['point', 'fake', 'news', 'forced', 'discuss', 'great', 'jobs', 'numbers', 'strong', 'economy', 'success', 'isis', 'border', 'much', 'else'], ['speaking', 'italy', 'morning'], ['spoke', 'yesterday', 'king', 'saudi', 'arabia', 'peace', 'middleeast', 'interesting', 'things', 'happening'], ['speaking', 'germany', 'france', 'morning']]\n",
      "3219\n"
     ]
    }
   ],
   "source": [
    "print(data[:5])\n",
    "print(len(data))\n",
    "print ('\\n')\n",
    "print(tokenized_tweets[:5])\n",
    "print(len(tokenized_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Embedding - From Words to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our doc2vec model on our sentences for 10 iterations and ensure use min_count=2 to filter out tweets with only 1 word. We represent all the documents in 100-dimensional vector space.\n",
    "\n",
    "Our resultant vectorized docuemnts are stored in model.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-03 12:49:58,402 : INFO : collecting all words and their counts\n",
      "2017-07-03 12:49:58,404 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-07-03 12:49:58,424 : INFO : collected 6503 word types and 3219 unique tags from a corpus of 3219 examples and 33749 words\n",
      "2017-07-03 12:49:58,425 : INFO : Loading a fresh vocabulary\n",
      "2017-07-03 12:49:58,438 : INFO : min_count=3 retains 2157 unique words (33% of original 6503, drops 4346)\n",
      "2017-07-03 12:49:58,439 : INFO : min_count=3 leaves 28446 word corpus (84% of original 33749, drops 5303)\n",
      "2017-07-03 12:49:58,448 : INFO : deleting the raw counts dictionary of 6503 items\n",
      "2017-07-03 12:49:58,451 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2017-07-03 12:49:58,453 : INFO : downsampling leaves estimated 26029 word corpus (91.5% of prior 28446)\n",
      "2017-07-03 12:49:58,454 : INFO : estimated required memory for 2157 words and 200 dimensions: 7104900 bytes\n",
      "2017-07-03 12:49:58,465 : INFO : resetting layer weights\n",
      "2017-07-03 12:49:58,568 : INFO : training model with 3 workers on 2157 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-07-03 12:49:59,645 : INFO : PROGRESS: at 3.84% examples, 211453 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-03 12:50:00,697 : INFO : PROGRESS: at 7.84% examples, 216993 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:01,727 : INFO : PROGRESS: at 11.84% examples, 220260 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-03 12:50:02,748 : INFO : PROGRESS: at 15.70% examples, 220300 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:03,816 : INFO : PROGRESS: at 19.68% examples, 220055 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-03 12:50:04,829 : INFO : PROGRESS: at 23.68% examples, 221808 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:05,845 : INFO : PROGRESS: at 27.53% examples, 221811 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:06,852 : INFO : PROGRESS: at 31.39% examples, 222030 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:07,893 : INFO : PROGRESS: at 35.39% examples, 222325 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-03 12:50:08,898 : INFO : PROGRESS: at 39.23% examples, 222506 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:09,908 : INFO : PROGRESS: at 42.94% examples, 221784 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:10,943 : INFO : PROGRESS: at 46.94% examples, 222148 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:11,974 : INFO : PROGRESS: at 50.94% examples, 222511 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:12,992 : INFO : PROGRESS: at 54.78% examples, 222426 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:14,037 : INFO : PROGRESS: at 58.78% examples, 222527 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:15,082 : INFO : PROGRESS: at 62.77% examples, 222604 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:16,102 : INFO : PROGRESS: at 66.77% examples, 222995 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:17,134 : INFO : PROGRESS: at 70.62% examples, 222742 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:18,188 : INFO : PROGRESS: at 74.62% examples, 222693 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:19,201 : INFO : PROGRESS: at 78.47% examples, 222676 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-03 12:50:20,206 : INFO : PROGRESS: at 82.17% examples, 222333 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:21,214 : INFO : PROGRESS: at 86.03% examples, 222385 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-03 12:50:22,237 : INFO : PROGRESS: at 90.03% examples, 222651 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:23,254 : INFO : PROGRESS: at 93.88% examples, 222595 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:24,301 : INFO : PROGRESS: at 97.88% examples, 222625 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-03 12:50:24,777 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-03 12:50:24,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-03 12:50:24,813 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-03 12:50:24,813 : INFO : training on 6749800 raw words (5850092 effective words) took 26.2s, 223010 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0223127   0.71794951  0.15652561 -0.3871288   0.09737334  0.30886981\n",
      " -0.1897698  -0.02318754  0.29577002 -0.25845578  0.85019886  0.15728363\n",
      "  0.66041511 -0.50196338 -0.01232568 -0.69066787 -0.11652973 -0.11894244\n",
      "  0.25488374 -0.26980492 -0.08240258 -0.33684728  0.03732453  0.17349607\n",
      "  0.12191286  0.03625823  0.04406116 -0.12207581  0.39899775 -0.17920512\n",
      " -0.13989277  0.24545549 -0.00747421 -0.31095013  0.23375721 -0.60718364\n",
      " -0.21391943  0.16231993  0.03919971  0.0108712  -0.45206833 -0.46321338\n",
      " -0.46140927 -0.07949995 -0.62248254  0.0677736   0.19140556  0.19491951\n",
      " -0.21051081 -0.02319764  0.50184739  0.5713349  -0.14749479  0.42342877\n",
      " -0.52857226 -0.41388413 -0.41674462  0.14050347  0.06123683  0.20868874\n",
      " -0.57156885  0.196704    0.10411382 -0.11350733  0.38653967 -0.48477373\n",
      " -0.3338708   0.11424053  0.36781299  0.01491879 -0.17283344  0.45260099\n",
      " -0.02258966  0.16506541 -0.50829226  0.04534959 -0.70939964  0.23371673\n",
      " -0.47035155 -0.23309872 -0.10950626 -0.09217826  0.18520956  0.10639726\n",
      "  0.36954418 -0.15825245 -0.43518826  0.41059914 -0.34507501 -0.66749734\n",
      " -0.28000584 -0.55067545 -0.11045953 -0.09892567  0.08659974 -0.34211367\n",
      "  0.17640242  0.56808645 -0.35402471 -0.19766127 -0.28867269  0.52646977\n",
      "  0.04909756 -0.24727604  0.86350465  0.0277087  -0.45079002 -0.04013845\n",
      "  0.68001974  0.22445646 -0.20080937 -0.14550319 -0.28825167 -0.34783128\n",
      "  0.01317219 -0.0043363   0.0127011   0.00588953 -0.01887885  0.21675649\n",
      " -0.10394394  0.10126881  0.33814332 -0.25676399 -0.04068553  0.30312324\n",
      " -0.86695182  0.19366877 -0.07219923 -0.29134271  1.22953594  0.07170969\n",
      " -0.20013116  0.01322109 -0.09608018  0.02648543  0.04670355  0.23683242\n",
      "  0.03689807 -0.08441685 -0.19887222 -0.14821632 -0.18092605 -0.46703553\n",
      " -0.5888837   0.51577252 -0.44417599 -0.06186676  0.10699115 -0.00205591\n",
      "  0.35994157 -0.26875976  0.04015722  0.0341026   0.33095056 -0.04952316\n",
      " -0.36556962  0.03148115 -0.69420582 -0.39253581 -0.318212    0.03777569\n",
      "  0.6019392  -0.10202194  0.1707385   0.07759415 -0.2382762   0.04770651\n",
      "  0.15185766 -0.53769588 -0.25057065  0.28037417  0.28529081  0.47187591\n",
      "  0.16683388  0.25755548 -0.21263161  0.60162461 -0.44603977  0.77670389\n",
      "  0.30604851 -0.22515996 -0.38087615 -0.47161189  0.09156749  0.28410885\n",
      " -0.63444483 -0.20629731 -0.07424626 -0.21848682  0.30961379  0.18632449\n",
      "  0.43564209  0.15715525 -0.18880691  0.29158205 -0.19939497  0.35215506\n",
      " -0.29810128 -0.30201176]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=200, min_count=3, iter=200)\n",
    "model.build_vocab(tag_tokenized)\n",
    "model.train(tag_tokenized, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print(model.docvecs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering our document vectors using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our model K-Means model to 100 distinct clusters and return those labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 50\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(model.docvecs)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in all other Metrics and preparing OLS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_jsn=[x for x in tweets_data.find({},\n",
    "                     {'favorite_count':1,'retweet_count':1,'created_at':1,'entities.hashtags':1,'entities.urls':1,\n",
    "                      'entities.media':1,'_id':0})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.io.json.json_normalize(data_jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['has_hashtags']=[len(a)>0 for a in df['entities.hashtags']]\n",
    "df['has_urls']=[len(a)>0 for a in df['entities.urls']]\n",
    "df['has_media']=1-df['entities.media'].isnull()\n",
    "df['dow']=df.created_at.str[:3]\n",
    "df['time']=df.created_at.str[11:13]\n",
    "df['time_group']=df.time.astype('int64').mod(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clusters=pd.concat([pd.Series(clusters),df,pd.Series(data)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities.hashtags</th>\n",
       "      <th>entities.media</th>\n",
       "      <th>entities.urls</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>has_hashtags</th>\n",
       "      <th>has_urls</th>\n",
       "      <th>has_media</th>\n",
       "      <th>dow</th>\n",
       "      <th>time</th>\n",
       "      <th>time_group</th>\n",
       "      <th>text</th>\n",
       "      <th>cluster_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jul 03 14:00:21 +0000 2017</td>\n",
       "      <td>[{'text': 'CharlieGard', 'indices': [22, 34]}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>27279</td>\n",
       "      <td>9279</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>If we can help little #CharlieGard, as per our...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Mon Jul 03 12:10:44 +0000 2017</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>56934</td>\n",
       "      <td>14786</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>At some point the Fake News will be forced to ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>Mon Jul 03 11:38:20 +0000 2017</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>35696</td>\n",
       "      <td>6392</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Will be speaking with Italy this morning!</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Mon Jul 03 11:19:08 +0000 2017</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>43428</td>\n",
       "      <td>13346</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Spoke yesterday with the King of Saudi Arabia ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>Mon Jul 03 11:00:56 +0000 2017</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>37902</td>\n",
       "      <td>7269</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Will be speaking with Germany and France this ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                      created_at  \\\n",
       "0        0  Mon Jul 03 14:00:21 +0000 2017   \n",
       "1       38  Mon Jul 03 12:10:44 +0000 2017   \n",
       "2       40  Mon Jul 03 11:38:20 +0000 2017   \n",
       "3       14  Mon Jul 03 11:19:08 +0000 2017   \n",
       "4       25  Mon Jul 03 11:00:56 +0000 2017   \n",
       "\n",
       "                                entities.hashtags entities.media  \\\n",
       "0  [{'text': 'CharlieGard', 'indices': [22, 34]}]            NaN   \n",
       "1                                              []            NaN   \n",
       "2                                              []            NaN   \n",
       "3                                              []            NaN   \n",
       "4                                              []            NaN   \n",
       "\n",
       "  entities.urls  favorite_count  retweet_count has_hashtags has_urls  \\\n",
       "0            []           27279           9279         True    False   \n",
       "1            []           56934          14786        False    False   \n",
       "2            []           35696           6392        False    False   \n",
       "3            []           43428          13346        False    False   \n",
       "4            []           37902           7269        False    False   \n",
       "\n",
       "   has_media  dow time  time_group  \\\n",
       "0          0  Mon   14           2   \n",
       "1          0  Mon   12           0   \n",
       "2          0  Mon   11           3   \n",
       "3          0  Mon   11           3   \n",
       "4          0  Mon   11           3   \n",
       "\n",
       "                                                text cluster_cat  \n",
       "0  If we can help little #CharlieGard, as per our...           0  \n",
       "1  At some point the Fake News will be forced to ...          38  \n",
       "2          Will be speaking with Italy this morning!          40  \n",
       "3  Spoke yesterday with the King of Saudi Arabia ...          14  \n",
       "4  Will be speaking with Germany and France this ...          25  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.columns=['cluster']+list(df.columns)+['text']\n",
    "df_clusters['cluster_cat']=pd.Categorical(df_clusters['cluster'])\n",
    "df_clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS modeling for Tweet Favorite Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance for OLS regressions for favorite_count with/without the inclusion of our clusters (var='cluster_cat') as an indepenent variable. We will also control for: Day of Week, Tweet Time of day, and if the tweet had linked urls,media or hashtags. Furthermore, we filter out tweets with 0 favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_baseline = smf.ols('favorite_count ~ dow + time_group + has_media + has_hashtags + has_urls', data=df_clusters[(df_clusters.favorite_count>0)]).fit()\n",
    "results_clusters = smf.ols('favorite_count ~ dow + time_group + has_media + has_hashtags + has_urls+cluster_cat', data=df_clusters[(df_clusters.favorite_count>0)]).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing OLS Results for both models. Baseline R2=.104 and Model w/ clusters R2=0.170. This indicates that clustering tweets into 100 distict topic groups improves our model fit. However, given the low explanatory power, there is still plenty of unexplained variance for the favorite count of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         favorite_count   R-squared:                       0.104\n",
      "Model:                            OLS   Adj. R-squared:                  0.101\n",
      "Method:                 Least Squares   F-statistic:                     34.55\n",
      "Date:                Mon, 03 Jul 2017   Prob (F-statistic):           2.48e-64\n",
      "Time:                        12:50:29   Log-Likelihood:                -36251.\n",
      "No. Observations:                3001   AIC:                         7.252e+04\n",
      "Df Residuals:                    2990   BIC:                         7.259e+04\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept             6.204e+04   2469.517     25.122      0.000      5.72e+04  6.69e+04\n",
      "dow[T.Mon]           -7163.1720   2936.697     -2.439      0.015     -1.29e+04 -1405.022\n",
      "dow[T.Sat]            -737.0418   3041.300     -0.242      0.809     -6700.294  5226.210\n",
      "dow[T.Sun]            -650.3155   3122.531     -0.208      0.835     -6772.843  5472.212\n",
      "dow[T.Thu]           -6001.0062   2837.174     -2.115      0.035     -1.16e+04  -437.996\n",
      "dow[T.Tue]           -3188.1632   2888.926     -1.104      0.270     -8852.648  2476.322\n",
      "dow[T.Wed]           -3719.2910   2876.341     -1.293      0.196     -9359.099  1920.517\n",
      "has_hashtags[T.True] -1.541e+04   2097.336     -7.347      0.000     -1.95e+04 -1.13e+04\n",
      "has_urls[T.True]     -2.107e+04   1781.503    -11.828      0.000     -2.46e+04 -1.76e+04\n",
      "time_group            1581.5696    716.049      2.209      0.027       177.570  2985.569\n",
      "has_media            -6947.9573   2435.667     -2.853      0.004     -1.17e+04 -2172.204\n",
      "==============================================================================\n",
      "Omnibus:                     2240.873   Durbin-Watson:                   0.739\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            78784.038\n",
      "Skew:                           3.169   Prob(JB):                         0.00\n",
      "Kurtosis:                      27.288   Cond. No.                         15.7\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         favorite_count   R-squared:                       0.170\n",
      "Model:                            OLS   Adj. R-squared:                  0.153\n",
      "Method:                 Least Squares   F-statistic:                     10.37\n",
      "Date:                Mon, 03 Jul 2017   Prob (F-statistic):           1.03e-81\n",
      "Time:                        12:50:29   Log-Likelihood:                -36136.\n",
      "No. Observations:                3001   AIC:                         7.239e+04\n",
      "Df Residuals:                    2942   BIC:                         7.275e+04\n",
      "Df Model:                          58                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept             8.273e+04   5966.750     13.866      0.000       7.1e+04  9.44e+04\n",
      "dow[T.Mon]           -5555.1320   2876.739     -1.931      0.054     -1.12e+04    85.493\n",
      "dow[T.Sat]            -298.9880   2971.014     -0.101      0.920     -6124.466  5526.490\n",
      "dow[T.Sun]            -204.1612   3060.874     -0.067      0.947     -6205.832  5797.510\n",
      "dow[T.Thu]           -4618.0051   2772.381     -1.666      0.096     -1.01e+04   817.998\n",
      "dow[T.Tue]           -2844.2799   2817.908     -1.009      0.313     -8369.551  2680.991\n",
      "dow[T.Wed]           -2258.8498   2809.859     -0.804      0.422     -7768.339  3250.640\n",
      "has_hashtags[T.True] -1.105e+04   2269.118     -4.871      0.000     -1.55e+04 -6603.069\n",
      "has_urls[T.True]     -1.997e+04   1930.606    -10.345      0.000     -2.38e+04 -1.62e+04\n",
      "cluster_cat[T.1]     -5.314e+04   7572.516     -7.018      0.000      -6.8e+04 -3.83e+04\n",
      "cluster_cat[T.2]     -6308.8483   7781.997     -0.811      0.418     -2.16e+04  8949.863\n",
      "cluster_cat[T.3]     -1.533e+04   7519.124     -2.038      0.042     -3.01e+04  -582.996\n",
      "cluster_cat[T.4]     -2.866e+04   1.24e+04     -2.312      0.021      -5.3e+04 -4354.139\n",
      "cluster_cat[T.5]     -3.545e+04   7952.631     -4.457      0.000      -5.1e+04 -1.99e+04\n",
      "cluster_cat[T.6]     -2.706e+04   8473.288     -3.193      0.001     -4.37e+04 -1.04e+04\n",
      "cluster_cat[T.7]     -2.113e+04   7548.341     -2.800      0.005     -3.59e+04 -6334.211\n",
      "cluster_cat[T.8]     -2.512e+04   8948.525     -2.807      0.005     -4.27e+04 -7575.656\n",
      "cluster_cat[T.9]     -3.019e+04   9600.289     -3.145      0.002      -4.9e+04 -1.14e+04\n",
      "cluster_cat[T.10]    -7675.7359   4.19e+04     -0.183      0.855     -8.98e+04  7.44e+04\n",
      "cluster_cat[T.11]    -3.055e+04   7811.502     -3.911      0.000     -4.59e+04 -1.52e+04\n",
      "cluster_cat[T.12]    -2.968e+04   6978.147     -4.253      0.000     -4.34e+04  -1.6e+04\n",
      "cluster_cat[T.13]    -3.362e+04   8528.038     -3.942      0.000     -5.03e+04 -1.69e+04\n",
      "cluster_cat[T.14]    -7164.6527   6834.188     -1.048      0.295     -2.06e+04  6235.623\n",
      "cluster_cat[T.15]    -2.139e+04   8378.937     -2.553      0.011     -3.78e+04 -4960.518\n",
      "cluster_cat[T.16]    -3.891e+04   2.99e+04     -1.301      0.193     -9.76e+04  1.97e+04\n",
      "cluster_cat[T.17]    -3.545e+04   7020.756     -5.049      0.000     -4.92e+04 -2.17e+04\n",
      "cluster_cat[T.18]    -1.213e+04   7881.851     -1.539      0.124     -2.76e+04  3320.582\n",
      "cluster_cat[T.19]      795.6156   7433.881      0.107      0.915     -1.38e+04  1.54e+04\n",
      "cluster_cat[T.20]    -2.295e+04   7910.701     -2.901      0.004     -3.85e+04 -7434.099\n",
      "cluster_cat[T.21]     -2.57e+04   1.33e+04     -1.927      0.054     -5.18e+04   448.485\n",
      "cluster_cat[T.22]    -1.895e+04   8120.400     -2.334      0.020     -3.49e+04 -3029.526\n",
      "cluster_cat[T.23]    -1.237e+04   1.12e+04     -1.099      0.272     -3.44e+04  9689.391\n",
      "cluster_cat[T.24]    -1.773e+04   7787.693     -2.277      0.023      -3.3e+04 -2460.080\n",
      "cluster_cat[T.25]    -1.526e+04   7024.803     -2.173      0.030      -2.9e+04 -1490.297\n",
      "cluster_cat[T.26]    -2.069e-11   2.23e-11     -0.926      0.355     -6.45e-11  2.31e-11\n",
      "cluster_cat[T.27]    -3.993e+04   6740.299     -5.924      0.000     -5.31e+04 -2.67e+04\n",
      "cluster_cat[T.28]    -4.093e+04   8658.571     -4.728      0.000     -5.79e+04  -2.4e+04\n",
      "cluster_cat[T.29]     2428.5173   9843.861      0.247      0.805     -1.69e+04  2.17e+04\n",
      "cluster_cat[T.30]    -2.894e+04   8867.166     -3.264      0.001     -4.63e+04 -1.16e+04\n",
      "cluster_cat[T.31]    -2.993e+04   7778.913     -3.847      0.000     -4.52e+04 -1.47e+04\n",
      "cluster_cat[T.32]    -2.889e+04   8473.420     -3.409      0.001     -4.55e+04 -1.23e+04\n",
      "cluster_cat[T.33]    -3.575e+04   7410.270     -4.824      0.000     -5.03e+04 -2.12e+04\n",
      "cluster_cat[T.34]    -1.014e+04   7391.631     -1.371      0.170     -2.46e+04  4355.805\n",
      "cluster_cat[T.35]    -3.986e+04   7731.178     -5.155      0.000      -5.5e+04 -2.47e+04\n",
      "cluster_cat[T.36]    -8760.1479    4.2e+04     -0.209      0.835      -9.1e+04  7.35e+04\n",
      "cluster_cat[T.37]    -4782.2533   7174.639     -0.667      0.505     -1.89e+04  9285.568\n",
      "cluster_cat[T.38]    -1.929e+04   7395.243     -2.608      0.009     -3.38e+04 -4788.671\n",
      "cluster_cat[T.39]    -1.576e+04   7954.400     -1.982      0.048     -3.14e+04  -166.860\n",
      "cluster_cat[T.40]    -2.542e+04   6088.094     -4.176      0.000     -3.74e+04 -1.35e+04\n",
      "cluster_cat[T.41]    -2.379e+04   7284.689     -3.266      0.001     -3.81e+04 -9506.171\n",
      "cluster_cat[T.42]     -4.42e+04   4.19e+04     -1.055      0.291     -1.26e+05  3.79e+04\n",
      "cluster_cat[T.43]    -1.417e+04   8035.692     -1.764      0.078     -2.99e+04  1582.729\n",
      "cluster_cat[T.44]    -2.244e+04   6533.878     -3.434      0.001     -3.53e+04 -9628.988\n",
      "cluster_cat[T.45]    -1.519e+04   8263.108     -1.839      0.066     -3.14e+04  1007.691\n",
      "cluster_cat[T.46]    -1.744e+04   9021.062     -1.933      0.053     -3.51e+04   247.805\n",
      "cluster_cat[T.47]    -2.286e+04   8587.542     -2.662      0.008     -3.97e+04 -6021.068\n",
      "cluster_cat[T.48]    -1.158e+04   8077.615     -1.433      0.152     -2.74e+04  4259.385\n",
      "cluster_cat[T.49]    -4.203e+04   7605.709     -5.526      0.000     -5.69e+04 -2.71e+04\n",
      "time_group            1472.7432    699.893      2.104      0.035       100.414  2845.073\n",
      "has_media            -5556.7045   2513.984     -2.210      0.027     -1.05e+04  -627.359\n",
      "==============================================================================\n",
      "Omnibus:                     2310.252   Durbin-Watson:                   0.885\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            92590.591\n",
      "Skew:                           3.273   Prob(JB):                         0.00\n",
      "Kurtosis:                      29.412   Cond. No.                     1.48e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.88e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "print(results_baseline.summary())\n",
    "print(results_clusters.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Taking a look at some of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The failing @nytimes writes false story after false story about me. They don't even call to verify the facts of a story. A Fake News Joke!\",\n",
       " '..under a magnifying glass, they have zero \"tapes\" of T people colluding. There is no collusion &amp; no obstruction. I should be given apology!',\n",
       " 'They made up a phony collusion with the Russians story, found zero proof, so now they go for obstruction of justice on the phony story. Nice',\n",
       " \"....it is very possible that those sources don't exist but are made up by fake news writers. #FakeNews is the enemy!\",\n",
       " 'It is my opinion that many of the leaks coming out of the White House are fabricated lies made up by the #FakeNews media.',\n",
       " \"Biggest story today between Clapper &amp; Yates is on surveillance. Why doesn't the media report on this? #FakeNews!\",\n",
       " 'The Russia-Trump collusion story is a total hoax, when will this taxpayer funded charade end?',\n",
       " 'Sally Yates made the fake media extremely unhappy today --- she said nothing but old news!',\n",
       " 'The two fake news polls released yesterday, ABC &amp; NBC, while containing some very positive info, were totally wrong in General E. Watch!',\n",
       " 'The Fake Media (not Real Media) has gotten even worse since the election. Every story is badly slanted. We have to hold them to the truth!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 15\n",
    "#General topic is Fake News\n",
    "[tweet for tweet in df_clusters[df_clusters.cluster_cat==15].text[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Will be speaking with Italy this morning!',\n",
       " 'Weekly AddressðŸ‡ºðŸ‡¸ #KatesLaw\\n#NoSanctuaryForCriminalsAct\\nStatement: https://t.co/I8cqKGDK2B https://t.co/00mao6Vk7R',\n",
       " 'The era of strategic patience with the North Korea regime has failed. That patience is over. We are working closelyâ€¦ https://t.co/MxN04V2Yn4',\n",
       " \"I am encouraged by President Moon's assurances that he will work to level the playing field for American workers, bâ€¦ https://t.co/DUZh6aMjJS\",\n",
       " 'Statement on House Passage of Kateâ€™s Law and No Sanctuary for Criminals Act. https://t.co/uPRy9XgK5A',\n",
       " '\"Mattis Says Trumpâ€™s Warning Stopped Chemical Weapons Attack In Syria\" https://t.co/XL9LLNKh8i',\n",
       " 'Join me LIVE with @VP, @SecretaryPerry, @SecretaryZinke and @EPAScottPruitt. \\n#UnleashingAmericanEnergy\\nhttps://t.co/hlM7F2BQD9',\n",
       " 'MAKE AMERICA SAFE AGAIN!\\n\\n#NoSanctuaryForCriminalsAct \\n#KatesLaw #SaveAmericanLives \\n\\nhttps://t.co/jbN4hPjqjS',\n",
       " 'The #AmazonWashingtonPost, sometimes referred to as the guardian of Amazon not paying internet taxes (which they should) is FAKE NEWS!',\n",
       " 'MAKE AMERICA GREAT AGAIN!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 40\n",
    "#General topic of NoSanctuaryForCriminalsAct\n",
    "[tweet for tweet in df_clusters[df_clusters.cluster_cat==40].text[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
